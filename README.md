# ğŸš€ Logistic Regression using Stochastic Gradient Descent (SGD)

A clear and engaging explanation of how logistic regression worksâ€”optimized via **SGD**, with math, intuition, and emoji-powered clarity! ğŸ“ŠğŸ¤–

---

## 1. Problem Setup ğŸ¯

- **Input Features**  
  `x = [xâ‚, xâ‚‚, ..., xâ‚™]`

- **Target Label**  
  `y âˆˆ {0,1}`  
  (e.g., spam vs. not spam)

---

## 2. Logistic Model ğŸ”

- **Linear Combination**  
  `z = w Â· x + b`

- **Sigmoid Activation**  
  `Å· = 1 / (1 + exp(-z))`

---

## 3. Prediction Rule ğŸ¤–

- If `Å· â‰¥ 0.5` â†’ predict **class 1**  
- Else â†’ predict **class 0**

---

## 4. Loss Function ğŸ“‰

**Binary Cross-Entropy**  
`L = - [ y log(Å·) + (1 - y) log(1 - Å·) ]`

---

## 5. Gradients ğŸ”

- Weight Gradient  
  `âˆ‚L/âˆ‚wâ±¼ = (Å· - y) Â· xâ±¼`

- Bias Gradient  
  `âˆ‚L/âˆ‚b = Å· - y`

---

## 6. What is SGD? âš¡

Unlike batch gradient descent, which uses the entire dataset, **SGD** updates parameters after each training example.

âœ¨ Benefits:
- Faster updates ğŸƒâ€â™‚ï¸  
- Escapes local minima ğŸš€  
- Efficient with large data ğŸ“Š

---

## 7. Update Rules ğŸ’¡

- Weight Update  
  `wâ±¼ := wâ±¼ - Î· Â· (Å· - y) Â· xâ±¼`

- Bias Update  
  `b := b - Î· Â· (Å· - y)`

Where `Î·` = learning rate ğŸšï¸

---

## 8. Example Walkthrough ğŸ§‘â€ğŸ«

| Parameter   | Value       |
|------------|-------------|
| `x`        | `[2, 3]`     |
| `y`        | `1`         |
| `w`        | `[0.1, -0.2]`|
| `b`        | `0.05`      |
| `Î·`        | `0.1`       |

**Step-by-step:**
- `z = 0.1Â·2 + (-0.2)Â·3 + 0.05 = -0.35`
- `Å· â‰ˆ 1 / (1 + exp(0.35)) â‰ˆ 0.413`
- Error: `Å· - y = -0.587`
- Update:
  - `wâ‚ = 0.1 + 0.1174 = 0.2174`
  - `wâ‚‚ = -0.2 + 0.1761 = -0.0239`
  - `b = 0.05 + 0.0587 = 0.1087`

---

## 9. Training Loop ğŸ”

- Repeat over dataset  
- Shuffle each epoch ğŸ”€  
- Iterate until convergence ğŸ“ˆ

---

## 10. Summary Table ğŸ“š

| Concept             | Formula                          | Emoji |
|---------------------|----------------------------------|--------|
| Linear Combination  | `z = w Â· x + b`                  | âš–ï¸     |
| Sigmoid Activation  | `Ïƒ(z) = 1 / (1 + exp(-z))`       | ğŸ”„     |
| Cross-Entropy Loss  | `L = -[ y log(Å·) + (1-y) log(1-Å·) ]` | ğŸ“‰ |
| Weight Gradient     | `(Å· - y) Â· xâ±¼`                   | ğŸ“     |
| Bias Gradient       | `Å· - y`                          | ğŸ“     |
| SGD Weight Update   | `wâ±¼ := wâ±¼ - Î· (Å· - y) xâ±¼`         | âš¡     |
| SGD Bias Update     | `b := b - Î· (Å· - y)`             | âš¡     |
| Learning Rate       | `Î·` â€“ step size                  | ğŸšï¸    |
